version: "2"
services:

  hive-server:
    image: hadoop:3.3.5-ubuntu-r1
    restart: always
    volumes:
      - "./configs/hive/log4j.properties:/opt/hive/conf/log4j.properties"
      - "./configs/hive/hive-env.sh:/opt/hive/conf/hive-env.sh"
      - "./test:/tmp/test"
      - "./configs/spark/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf"
      - "./configs/spark/spark-env.sh:/opt/spark/conf/spark-env.sh"
      - spark_staging:/opt/hadoop/.sparkStaging
      - hive_warehouse:/data
    env_file:
      - ./hive.cfg
    command: ["hive", "--service", "hiveserver2"]
    environment:
      PATH: '/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/hadoop/bin:/opt/hive/bin:/opt/spark/bin'
      HADOOP_HOME: "/opt/hadoop"
      HADOOP_CONF_DIR: "/opt/hive/conf"
    ports:
      - "10000:10000"
    networks:
      hadoop:

  hive-metastore:
    image: hadoop:3.3.5-ubuntu-r1
    restart: always
    env_file:
      - ./hive.cfg
    volumes:
      - "./configs/hive/log4j.properties:/opt/hive/conf/log4j.properties"
      - "./configs/hive/hive-env.sh:/opt/hive/conf/hive-env.sh"
      - "./test:/tmp/test"
      - "./configs/spark/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf"
      - "./configs/spark/spark-env.sh:/opt/spark/conf/spark-env.sh"
    command: ["hive", "--service", "metastore"]
    environment:
      PATH: '/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/hadoop/bin:/opt/hive/bin:/opt/spark/bin'
      HADOOP_HOME: "/opt/hadoop"
      HADOOP_CONF_DIR: "/opt/hive/conf"
    ports:
      - "9083:9083"
    networks:
      hadoop:

  hive-metastore-init:
    image: hadoop:3.3.5-ubuntu-r1
    restart: no
    depends_on:
      - hive-metastore-postgresql
    env_file:
      - ./hive.cfg
    volumes:
      - "./configs/hive/log4j.properties:/opt/hive/conf/log4j.properties"
    command: ["/opt/hive/bin/schematool", "-dbType", "postgres", "-initSchema", "--verbose" ]
    environment:
      PATH: '/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/hadoop/bin:/opt/hive/bin:/opt/spark/bin'
      HADOOP_HOME: "/opt/hadoop"
      HADOOP_CONF_DIR: "/opt/hive/conf"
    networks:
      hadoop:

  hive-metastore-postgresql:
    image: docker-registry.services.linux2be.com:443/devops-f/devops/postgresql:15.4.0.c
    restart: always
    environment:
       POSTGRESQL_USERNAME: "postgres"
       POSTGRESQL_PASSWORD: "phahMMMddd999h7uutheePighMMMdsdsdss"
       POSTGRESQL_DATABASE: "hive-metastore"
       CONSUL_AGENT: "false"
    ports:
      - "35432:5432"
    volumes:
      - "./configs/postgres:/opt/bitnami/postgresql/conf/conf.d"
      - "postgres-15-2-bitnami_data:/bitnami/postgresql"
    networks:
      hadoop:
        aliases:
          - postgres

  namenode:
    image: hadoop:3.3.5-ubuntu-r1
    restart: always
    hostname: namenode
    volumes:
      - ./Makefile:/opt/hadoop/Makefile
      - hdfs_datanode:/data/hdfs
      - "./configs/spark/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf"
      - "./configs/spark/spark-env.sh:/opt/spark/conf/spark-env.sh"
    ports:
      - 9870:9870
    env_file:
      - ./hadoop.cfg
    environment:
      ENSURE_NAMENODE_DIR: "/data/hdfs/namenode"
      # ENSURE_NAMENODE_CLUSTERID
      HIVE_HOME: "/opt/hive"
    command: ["hdfs", "namenode"]
    networks:
      hadoop:

  datanode:
    image: hadoop:3.3.5-ubuntu-r1
    restart: always
    hostname: datanode
    volumes:
      - hdfs_datanode:/data/hdfs
      - spark_staging:/opt/hadoop/.sparkStaging
      - "./configs/spark/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf"
      - "./configs/spark/spark-env.sh:/opt/spark/conf/spark-env.sh"
    #entrypoint: tail -f /dev/null
    command: [ "hdfs", "datanode" ]
    env_file:
      - ./hadoop.cfg
    networks:
      hadoop:
  resourcemanager:
    image: hadoop:3.3.5-ubuntu-r1
    restart: always
    hostname: resourcemanager
    volumes:
      - spark_staging:/opt/hadoop/.sparkStaging
      - "./configs/spark/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf"
      - "./configs/spark/spark-env.sh:/opt/spark/conf/spark-env.sh"
    command: [ "yarn", "resourcemanager" ]
    ports:
      - 8088:8088
    env_file:
      - ./hadoop.cfg
    networks:
      hadoop:

  nodemanager:
    image: hadoop:3.3.5-ubuntu-r1
    restart: always
    hostname: nodemanager
    volumes:
      - spark_staging:/opt/hadoop/.sparkStaging
      - "./configs/spark/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf"
      - "./configs/spark/spark-env.sh:/opt/spark/conf/spark-env.sh"

    command: [ "yarn", "nodemanager" ]
    env_file:
      - ./hadoop.cfg
    networks:
      hadoop:

  spark-master:
    image: hadoop:3.3.5-ubuntu-r1
    restart: always
    hostname: spark-master
    volumes:
        - "./configs/spark/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf"
        - "./configs/spark/spark-env.sh:/opt/spark/conf/spark-env.sh"
    command: [ "spark-class", "org.apache.spark.deploy.master.Master" ]
    ports:
      - "8080:8080"
      - "7077:7077"
    env_file:
      - ./hadoop.cfg
    environment:
      PATH: "/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/hadoop/bin:/opt/spark/bin"
      HADOOP_HOME: "/opt/hadoop"
    networks:
      hadoop:

  spark-worker:
    image: hadoop:3.3.5-ubuntu-r1
    restart: always
    hostname: spark-worker
    volumes:
        - "./configs/spark/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf"
        - "./configs/spark/spark-env.sh:/opt/spark/conf/spark-env.sh"
    command: [ "spark-class", "org.apache.spark.deploy.worker.Worker" , "spark://spark-master:7077"]
    depends_on:
      - spark-master
    ports:
      - "8081:8081"
    env_file:
      - ./hadoop.cfg
    environment:
      PATH: "/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/hadoop/bin:/opt/spark/bin"
      HADOOP_HOME: "/opt/hadoop"
    networks:
      hadoop:

  historyserver:
    image: hadoop:3.3.5-ubuntu-r1
    restart: always
    command: [ "yarn", "historyserver" ]
    volumes:
      - hdfs_historyserver:/data/hdfs/
    ports:
      - 8188:8188
    env_file:
      - ./hadoop.cfg
    environment:
      PATH: "/opt/spark/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/hadoop/bin"

    networks:
      hadoop:

volumes:
  postgres-15-2-bitnami_data:
  hdfs_datanode:
  hdfs_historyserver:
  spark_staging:
  hive_warehouse:

networks:
  hadoop: